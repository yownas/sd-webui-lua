sd-webui-lua links: <a href="http://github.com/yownas/sd-webui-lua/">Github</a> <a href="https://github.com/yownas/sd-webui-lua/wiki">Wiki</a> <a href="https://github.com/scoder/lupa">Lupa</a>

<p>
<h2>Functions:</h2>
<p>
<b>ui.out(string):</b><br>
Write string to the Output box.
</p>
<p>
<b>ui.clear():</b><br>
Clear Output box.
</p>
<p>
<b>sd.empty_latent():</b><br>
Get a latent filled with zeroes. (Not used at the moment)
</p>
<p>
<b>sd.pipeline(p):</b><br>
Deconstructed pipeline from the webui.Generate picture from processing object.
</p>
<p>
<b>sd.process(string):</b><br>
Webui pipeline, generate image from a prompt-string or processing object.
</p>
<p>
<b>sd.getp():</b><br>
</p>
Returns a default processing object (see below).
<p>
<b> sd.cond(string):</b><br>
Run prompt string through clip.
</p>
<p>
<b>sd.negcond(string)</b><br>
Run negative prompt string through clip. (These are unfortunately slightly different at the momemt)
</p>
<p>
<b>sd.sample(p, cond, negcond):</b><br>
Turn noise into something that can get turned into an image. Takes a Processing object, a cond and a negcond value. Cond and negcond can also be Null, string or a tensor from sd.textencode().
</p>
<p>
<b>sd.vae(latent):</b><br>
Variational auto-envoder.
</p>
<p>
<b>sd.toimage(latent):</b><br>
Last step to get an image after the vae
</p>

<p>
<b>sd.textencode(string):</b><br>
Get a tensor from Clips text encode
</p>
<p>
<b>sd.clip2negcond(text encode):</b><br>
Convert tensor to a negative conditioning used by functions from the webui.
</p>
<p>
<b>sd.negcond2cond(negcond):</b><br>
Convert a negative conditioning to conditioning used by functions from the webui. The regular prompt and the negative prompt is treated slightly different internally, this is why this is needed.
</p>
<p>
<b>sd.getsamplers():</b><br>
Get list of samplers.
</p>

<p>
<b>ui.clear():</b><br>
Clear everything.
</p>
<p>
<b>ui.console(string):</b><br>
Print to console. 
</p>
<p>
<b>ui.out(string):</b><br>
Print to Output box.
</p>
<p>
<b>ui.gallery.add(image):</b><br>
Add image to Gallery
</p>
<p>
<b>ui.gallery.addc(image, string):</b><br>
Add image with caption to Gallery.
</p>
<p>
<b>ui.gallery.clear():</b><br>
Clear the gallery.
</p>
<p>
<b>ui.gallery.del(int):</b><br>
Delete image from galler. (Starts at 1 since this is Lua.)
</p>
<p>
<b>ui.gallery.getgif(duration):</b><br>
Get a gif from the images in the gallery. Show each image for "duration" ms.
</p>
<p>
<b>ui.image.save(image, name):</b><br>
Same image.
</p>
<p>
<b>ui.status(text):</b><br>
Update status-text under the buttons during run.
</p>
<p>
<b>torch_clamp(v1, min, max):</b><br>
Clamp vector v1 between min and max.
</p>
<p>
<b>torch.lerp(v1, v2, weight):</b><br>
Linear interpolation of v1 and v2, by weight. v1 + weight * (v2 - v1)
</p>
<p>
<b>torch.add(v1, v2):</b><br>
Add v2 (vector or float) to v1.
</p>
<p>
<b>torch.sub(v1, v2):</b><br>
Subtract v2 (vector or float) from v1.
</p>
<p>
<b>torch.mul(v1, v2):</b><br>
Multiply v2 (vector or float) with v1.
</p>
<p>
<b>torch.div(v1, v2):</b><br>
Divide v1 with v2 (vector or float).
</p>
<p>
<b>torch.size(v1):</b><br>
Return the size of vector v1
</p>
<p>
<b>torch.new_zeros(size):</b><br>
Take a Lua table, size, and create a zero-filled tensor.
</p>
<p>
<b>torch.max(v):</b><br>
Return the max value in v.
</p>
<p>
<b>torch.min(v):</b><br>
Return the min value in v.
</p>
<p>
<b>torch.f2t(tensor):</b><br>
Return a tensor from a float.
</p>
<p>
<b>torch.t2f(tensor):</b><br>
Return a float from a tensor.
</p>

<p>
<h2>Default Processing-object:</h2><br>
<pre>
p = StableDiffusionProcessingTxt2Img(
 sd_model=shared.sd_model,
 outpath_samples=shared.opts.outdir_samples or shared.opts.outdir_txt2img_samples,
 outpath_grids=shared.opts.outdir_grids or shared.opts.outdir_txt2img_grids,
 prompt='',
 styles=[],
 negative_prompt='',
 seed=-1,
 subseed=-1,
 subseed_strength=0,
 seed_resize_from_h=0,
 seed_resize_from_w=0,
 seed_enable_extras=True,
 sampler_name='Euler a',
 batch_size=1,
 n_iter=1,
 steps=20,
 cfg_scale=7,
 width=512,
 height=512,
 restore_faces=False,
 tiling=False,
 enable_hr=False,
 denoising_strength=0,
 hr_scale=0,
 hr_upscaler=None,
 hr_second_pass_steps=0,
 hr_resize_x=0,
 hr_resize_y=0,
 override_settings=[],
)
</pre>
</p>
